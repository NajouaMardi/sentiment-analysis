{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwl64CJD9bHF0wLVtaLBLt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NajouaMardi/sentiment-analysis/blob/master/app/Voting_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, make_scorer"
      ],
      "metadata": {
        "id": "gvZGtoQQMWru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('processed_reviews_100k_cleaned.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Fb2gHGR8Mibv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape \",df.shape)\n",
        "print(\"checking if data is missing\",df.isnull().sum())\n",
        "print(\"data types of columns\",df.dtypes)"
      ],
      "metadata": {
        "id": "YQVSy5cgMklk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Summary statistics for numeric columns\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "RWTt2wH2Mk5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the value counts of relevant categorical columns (e.g., \"rating\", \"helpfulness\")\n",
        "df['rating'].value_counts()"
      ],
      "metadata": {
        "id": "GYpp1FbfMmly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rating_counts = df[\"rating\"].value_counts().sort_index()\n",
        "rating_percentages = (rating_counts / rating_counts.sum()) * 100\n",
        "\n",
        "# Display nicely\n",
        "rating_distribution = pd.DataFrame({\n",
        "    \"count\": rating_counts,\n",
        "    \"percentage\": rating_percentages.round(2)\n",
        "})\n",
        "\n",
        "print(rating_distribution)"
      ],
      "metadata": {
        "id": "ux1gRUDqMo0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the dataset has a rating column or sentiment labels, explore their distribution\n",
        "df['rating'].value_counts().plot(kind='bar', title='Distribution of Ratings')"
      ],
      "metadata": {
        "id": "UyB3Mhn_Mqsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_counts = df[\"sentiment\"].value_counts()\n",
        "sentiment_percentages = (sentiment_counts / len(df)) * 100\n",
        "sentiment_summary = pd.DataFrame({\n",
        "    \"count\": sentiment_counts,\n",
        "    \"percentage\": sentiment_percentages.round(2)\n",
        "})\n",
        "print(sentiment_summary)"
      ],
      "metadata": {
        "id": "NF9qBYoTMsdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "#stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and numbers\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Tokenize and remove stop words\n",
        "    tokens = [word for word in text.split() if word not in stop_words]\n",
        "    # Stemming\n",
        "    #stemmed = [stemmer.stem(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply to full_text (title+text)\n",
        "df['clean_full_text'] = df['full_text'].apply(preprocess)\n"
      ],
      "metadata": {
        "id": "XFDhumiSMvSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['clean_full_text']\n",
        "y = df['sentiment']"
      ],
      "metadata": {
        "id": "U3DYf0BvMw6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ddzJefjMScU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Split your data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define classifiers with fixed parameters (can be tuned indirectly)\n",
        "nb = MultinomialNB(alpha=0.1)\n",
        "svm = LinearSVC(class_weight='balanced', max_iter=3000)\n",
        "lr = LogisticRegression(solver='saga', class_weight='balanced', max_iter=3000)\n",
        "\n",
        "# Voting ensemble\n",
        "voting = VotingClassifier(estimators=[\n",
        "    ('nb', nb),\n",
        "    ('svm', svm),\n",
        "    ('lr', lr)\n",
        "], voting='hard')  # or 'soft' if using models that support predict_proba\n",
        "\n",
        "# Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('chi2', SelectKBest(score_func=chi2)),\n",
        "    ('vote', voting)\n",
        "])\n",
        "\n",
        "# Param grid (mostly on preprocessing, not on base classifiers directly)\n",
        "param_grid = {\n",
        "    'tfidf__max_features': [4000, 5000],\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "    'chi2__k': [2000, 3000],\n",
        "    # Optional tuning for LogisticRegression\n",
        "    'vote__lr__C': [0.1, 1, 10],\n",
        "    # Optional tuning for SVM\n",
        "    'vote__svm__C': [0.1, 1, 10],\n",
        "    # Optional tuning for NB\n",
        "    'vote__nb__alpha': [0.01, 0.1, 1.0],\n",
        "}\n",
        "\n",
        "# Cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# GridSearch\n",
        "grid = GridSearchCV(pipeline, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Evaluation\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "print(\"Best parameters:\")\n",
        "for k, v in grid.best_params_.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "print(\"Best CV Accuracy:\", grid.best_score_)\n",
        "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Test F1 Macro:\", f1_score(y_test, y_pred, average='macro'))\n",
        "print(\"Test F1 Weighted:\", f1_score(y_test, y_pred, average='weighted'))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ]
    }
  ]
}